\begin{theo}[Estimation and Fitting Problems]{EstFitProblems}
    Estimation and fitting problems are optimization problems with a special objective, namely a least squares objective. We define the estimation problem as
    \begin{equation*}
        \min_{x \in \R^n} \frac{1}{2} \| \eta - M(x) \|^2_2,
    \end{equation*}
    where $\eta \in \R^m$ is the measurement vector, $M: \R^n \to \R^m$ is the model function, and $x \in \R^n$ is the parameter vector.  Many models in estimation and fitting problems are linear functions of $x$. If $M$ is linear, $M(x) = Jx$, then $f(x) = \frac{1}{2} \| \eta - Jx \|^2_2$ which is a convex function, as $\nabla^2 f(x) = J^T J \succeq 0$. Therefore local minimizers are found by:
    \begin{align*}
        \nabla f(x) = 0 
            &\Leftrightarrow J^T J x^* - J^T \eta = 0 \\
            &\Leftrightarrow x^* = \underbrace{{(J^T J)}^{-1} J^T}_{J^+} \eta
    \end{align*}
    \vspace*{-0.5cm}
\end{theo}

\begin{theo}[Pseudo-inverse]{PseudoInverse}
    $J^+$ is called the pseudo-inverse and is a generalization of the inverse matrix. If $J^T J \succ 0$, $J^+$ is given by 
    \begin{equation*}
        J^+ = {(J^T J)}^{-1} J^T.
    \end{equation*}
    So far, ${(J^T J)}^{-1}$ is only defined if $J^T J \succ 0$. THis holds if and only if $rank(J) = n$, i\@.e\@. if the collumds of $J$ are linearly independent. 
\end{theo}