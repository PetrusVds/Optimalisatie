% \begin{theo}[Estimation and Fitting Problems]{EstFitProblems}
%     Estimation and fitting problems are optimization problems with a special objective, namely a least squares objective. We define the estimation problem as
%     \begin{equation*}
%         \min_{x \in \R^n} \frac{1}{2} \| \eta - M(x) \|^2_2,
%     \end{equation*}
%     where $\eta \in \R^m$ is the measurement vector, $M: \R^n \to \R^m$ is the model function, and $x \in \R^n$ is the parameter vector.  
%     % Many models in estimation and fitting problems are linear functions of $x$. If $M$ is linear, $M(x) = Jx$, then $f(x) = \frac{1}{2} \| \eta - Jx \|^2_2$ which is a convex function, as $\nabla^2 f(x) = J^T J \succeq 0$. Therefore local minimizers are found by:
%     % \begin{align*}
%     %     \nabla f(x) = 0 
%     %         &\Leftrightarrow J^T J x^* - J^T \eta = 0 \\
%     %         &\Leftrightarrow x^* = \underbrace{{(J^T J)}^{-1} J^T}_{J^+} \eta
%     % \end{align*}
%     % \vspace*{-0.5cm}
% \end{theo}

% \begin{theo}[Pseudo-inverse]{PseudoInverse}
%     $J^+$ is called the pseudo-inverse and is a generalization of the inverse matrix. If $J^T J \succ 0$, $J^+$ is given by 
%     \begin{equation*}
%         J^+ = {(J^T J)}^{-1} J^T.
%     \end{equation*}
%     So far, ${(J^T J)}^{-1}$ is only defined if $J^T J \succ 0$. This holds if and only if $rank(J) = n$, i\@.e\@. if the collumds of $J$ are linearly independent. 
% \end{theo}

% \begin{theo}[Moore-Penrose Pseudo Inverse]{MoorePenrose}
%     Assume $J \in \R^{m \times n}$ and that the singular value decomposition (SVD) of $J$ is given by $J = U \Sigma V^T$. Then, the Moore-Penrose pseudo-inverse $J^+$ is given by
%     \begin{equation*}
%         J^+ = V S^+ U^T,
%     \end{equation*}
%     where for
%     \begin{equation*}
%     S = 
%         \left[
%         \begin{array}{ccccccc}
%         \sigma_1 &  &  &  &  &  \\
%          & \sigma_2 &  &  &  &  \\
%          &  & \ddots &  &  & \\
%          &  &  & \sigma_r &  & \\
%          &  &  &  & 0 & & \\
%          &  &  &  & & \ddots & \\
%          &  &  &  & & & 0 \\
%         \hline
%         0 & \cdots & \cdots & 0 & \cdots & \cdots & 0
%         \end{array}
%         \right]
%     \quad \text{holds} \quad
%     S^+ = 
%         \left[
%         \begin{array}{ccccccc|c}
%             \sigma_1^{-1} &  &  &  &  & & & 0\\
%             & \sigma_2^{-1} &  &  &  & &  & \vdots \\
%             &  & \ddots &  &  & & & \vdots \\
%             &  &  & \sigma_r^{-1} & & & & 0\\
%             &  &  &  & 0 & & & \vdots\\
%             &  &  &  & & \ddots &  & \vdots\\
%             &  &  &  & & & 0 & 0\\
%         \end{array}
%         \right]
%     \end{equation*}
%     \textbf{Note:} When $JJ^T$ is invertible, the pseudo-inverse is given by $J^+ = J^T {(J J^T)}^{-1}$.
% \end{theo}

% % \begin{pro}[Moore Penrose Pseudo Inverse]{proMoorePenrose}
% %     If matrix $A \in \R^{m \times n}$ 
% %     \begin{itemize}
% %         \item 
% %             is non-singular then 
% %             \begin{equation*}
% %                 A^+ = A^{-1},
% %             \end{equation*}
% %         \item 
% %             has full column rank, that is $r = n$, then 
% %             \begin{equation*}
% %                 A^+A = VV^T = I,
% %             \end{equation*}
% %             i\@.e\@. $A^+$ is a left inverse of $A$,
% %         \item 
% %             has full row rank, that is $r = m$, then 
% %             \begin{equation*}
% %                 AA^+ = UU^T = I,
% %             \end{equation*}
% %             i\@.e\@. $A^+$ is a right inverse of $A$.
% %     \end{itemize}
% % \end{pro}

% \begin{theo}[Regularization for (Linear) Least Squares]{RegLeastSquares}
%     The minimum norm solution can be approximated by a regularized problem
%     \begin{equation*}
%         \min_{x \in \R^n} \frac{1}{2} \| \eta - Jx \|^2_2 + \frac{\epsilon}{2} \| x \|^2_2,
%     \end{equation*}
%     with small $\epsilon > 0$, to get a unique solution:
%     \begin{align*}
%         \nabla f(x) 
%             &= J^{T}Jx - J^T\eta + \epsilon x \\
%             &= (J^TJ + \epsilon I)x - J^T\eta \\
%         x^* &= (J^TJ + \epsilon I)^{-1}J^T\eta
%     \end{align*}
%     \vspace{-0.5cm}
% \end{theo}

% \newpage

% \begin{lem}[$\epsilon \to 0$]{epsilon0}
%     For $\epsilon \to 0$, the optimal solution $x^*$ of the regularized problem converges to the Moore-Penrose pseudo-inverse, i\@.e\@. $\lim_{\epsilon \to 0} (J^TJ + \epsilon I)^{-1}J^T\eta = J^+$.
%     % \vspace{-0.3cm}
% \end{lem}